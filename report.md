### Báo cáo kết quả các bài tập đã làm được của nhóm 
| Bài tập | Báo cáo |
|:---|:---|
| Exercise-1 | Nhóm hoàn chỉnh file *main.py* trong folder của bài tập này. Nhóm đã sử dụng các thư viện:<br>- *requests* để gửi yêu cầu HTTP và tải các file từ các URL; <br>- *os* để quản lý đường dẫn và tạo thư mục download;<br>- *zipfile* để giải nén các file *zip* được tải về;<br>- *shutil* để xóa các thư mục và file.<br> Các hàm mà nhóm đã xây dựng:<br>- *download_file* để tách tên file từ URL, tải file đótừ URL đó về lưu vào thư mục *downloads*, nhóm sử dụng stream=True để tải các file lớn;<br>- *upzip_delete* để giải nén tất cả cá file zip trong thư mục *downloads* sau đó xóa tất cả các file zip đó;<br>- *main* để tạo thu mục *download* và bắt đầu vòng lặp duyệt qua các URL và tải file.<br>Khi chạy code thì đã tải và giải nén thành công các file trong các URL ban đầu vào folder *downloads* như đề bài yêu cầu, ngoại trừ file *Divvy_Trips_2220_Q1.zip* là bị lỗi không giải nén được. ![](/images/ex1.png) |
| Exercise-2 | Nhóm đã hoàn thành file *main.py* bằng cách sử dụng các thư viện:<br>- *requests* để tải nội dung trang web;<br>- *BeautifulSoup* để phân tích cấu trúc HTML và tìm file có thời gian "2024-01-19 10:27";<br>- *pandas* để đọc file CSV và tìm giá trị lớn nhất của "HourlyDryBulbTemperature".<br>Các hàm chính nhóm đã xây dựng:<br>- *make_download_dir* để tạo thư mục downloads nếu chưa tồn tại;<br>- *download_file* để tải file về từ URL tìm được;<br>- *highest_HourlyDryBulbTemperature* để tìm giá trị nhiệt độ cao nhất;<br>- *main* để điều phối toàn bộ quy trình từ tìm kiếm đến hiển thị kết quả.<br>Kết quả chạy code đã tìm đúng file và xác định được giá trị nhiệt độ cao nhất như yêu cầu. ![](/images/ex2.png) |
| Exercise-3 | Nhóm đã giải quyết bài tập bằng cách sử dụng:<br>- *requests* để tải file từ AWS Common Crawl mà không cần dùng boto3;<br>- *gzip* và *io* để xử lý file nén trong bộ nhớ;<br>- Xử lý encoding UTF-8 để đảm bảo hiển thị đúng các ký tự đặc biệt.<br>Các hàm quan trọng:<br>- *download_and_extract_gz_in_memory* tải và giải nén file gz đầu tiên;<br>- *stream_gz_lines* đọc file thứ 2 theo từng dòng để tiết kiệm bộ nhớ;<br>- *main* điều phối quá trình và xử lý lỗi.<br>Code đã chạy thành công, tải được file và in ra nội dung từng dòng của file lớn mà không gặp vấn đề về bộ nhớ. ![](/images/ex3.png) |
| Exercise-4 | Nhóm đã hoàn thành bài tập với các thư viện:<br>- *glob* để tìm kiếm đệ quy các file JSON;<br>- *json* để đọc dữ liệu từ file;<br>- *csv* để ghi dữ liệu đã được làm phẳng.<br>Các hàm chính:<br>- *find_all_json_files* tìm tất cả file JSON trong thư mục data;<br>- *flatten_json* xử lý làm phẳng cấu trúc JSON phức tạp, đặc biệt với trường hợp tọa độ địa lý;<br>- *convert_json_to_csv* chuyển đổi từ JSON sang CSV;<br>- *main* điều phối toàn bộ quá trình.<br>Code đã chạy thành công, tìm và chuyển đổi tất cả file JSON sang CSV với cấu trúc dữ liệu phẳng. ![](/images/ex4.png) |
| Exercise-5 | Nhóm đã xây dựng hệ thống cơ sở dữ liệu Postgres với:<br>- *psycopg2* để kết nối và tương tác với Postgres;<br>- Thiết kế schema với đầy đủ khóa chính, khóa ngoại và indexes;<br>- Cơ chế import dữ liệu từ CSV vào các bảng tương ứng.<br>Các thành phần chính:<br>- *create_tables* tạo 3 bảng customers, products, transactions với đầy đủ ràng buộc;<br>- *ingest_data* nhập dữ liệu từ các file CSV vào các bảng;<br>- *main* thiết lập kết nối và điều phối quá trình.<br>Hệ thống đã chạy thành công, tạo đúng cấu trúc bảng và nhập đầy đủ dữ liệu. ![](/images/ex5.png) |
| Pipeline | Nhóm thống nhất sử dụng Airflow để triển khai pipeline, dùng PostgreSQL để làm cơ sở dữ liệu. Trước tiên, nhóm đã xây dựng file docker-compose để định nghĩa các service có liên quan tới container, nhóm thống nhất định nghĩa service postgres của Exercise-5 vào chúng file docker-compose cho pipeline. Có 1 điểm khác biệt so với khi làm bài tập cá nhân là nhóm đã giới hạn lại số URL để tải file ở Exercise-1 xuống còn 1, ở Exercise-3 nhóm giới hạn chỉ in ra 10000 dòng của file thứ 2 để đỡ tốn thời gian. Sau đó chạy container của pipeline ![](/images/pipeline.png) |
|LAB 08||
|Case2| 1. `simple_dag_local.py`<br>- DAG đơn giản với 3 task: `start`, `process`, `end`.<br>- Mỗi task sử dụng `PythonOperator` để in ra thông báo tương ứng.<br>- Mục tiêu là minh họa cấu trúc cơ bản của một DAG và cách các task được kết nối tuần tự.<br> 2. `complex_dag_local.py`<br>- DAG mô phỏng quy trình xử lý dữ liệu và huấn luyện mô hình học máy.<br>- Gồm các task: `start`, `extract_data`, `transform_data`, `load_data`, `train_model`, `evaluate_model`, `end`.<br>- Cho thấy cách thiết kế một pipeline có nhánh và nhiều bước xử lý phức tạp hơn.<br> 3. `sensor_local.py`<br>- Sử dụng `ExternalTaskSensor` để chờ một DAG khác hoàn thành trước khi thực hiện task tiếp theo.<br>- Gồm các task: `wait_for_external_dag` và `process_after_wait`.<br>- Minh họa cách đồng bộ hóa giữa các DAG trong Airflow.<br> 4. `miai_dag.py`<br>- DAG chính mô phỏng quy trình xử lý dữ liệu AI thực tế.<br>- Gồm các task: `start`, `crawl_data`, `preprocess_data`, `train_model`, `evaluate_model`, `deploy_model`, `end`.<br>- Mô phỏng toàn bộ vòng đời dự án AI: từ thu thập dữ liệu đến triển khai mô hình. Quy trình khởi chạy pipeline:<br>1. Cài đặt Apache Airflow theo hướng dẫn chính thức.<br>2. Đặt các file `.py` vào thư mục `dags/` trong thư mục cài đặt Airflow.<br>3. Khởi động `airflow scheduler` và `airflow webserver`.<br>4. Truy cập tại [http://localhost:8080](http://localhost:8080) để theo dõi và chạy các DAG.<br>5. 3 task đầu là ví dụ để hiểu airflow ở mức cơ bản, Task 4 sẽ cào dữ liệu chứng khoáng từ trang vndirect từ ngày 1/1/2000<br>6. Sau khi dữ liệu cào được sẽ được lưu vào máy local và gửi từ gmail 1 tới gmail2 vào 3g chiều mỗi ngày (gmail cài đặt trong file miai_dag.py)|