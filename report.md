### Báo cáo kết quả các bài tập đã làm được của nhóm 
| Bài tập | Báo cáo |
|:---|:---|
| Exercise-1 | Nhóm hoàn chỉnh file *main.py* trong folder của bài tập này. Nhóm đã sử dụng các thư viện:<br>- *requests* để gửi yêu cầu HTTP và tải các file từ các URL; <br>- *os* để quản lý đường dẫn và tạo thư mục download;<br>- *zipfile* để giải nén các file *zip* được tải về;<br>- *shutil* để xóa các thư mục và file.<br> Các hàm mà nhóm đã xây dựng:<br>- *download_file* để tách tên file từ URL, tải file đótừ URL đó về lưu vào thư mục *downloads*, nhóm sử dụng stream=True để tải các file lớn;<br>- *upzip_delete* để giải nén tất cả cá file zip trong thư mục *downloads* sau đó xóa tất cả các file zip đó;<br>- *main* để tạo thu mục *download* và bắt đầu vòng lặp duyệt qua các URL và tải file.<br>Khi chạy code thì đã tải và giải nén thành công các file trong các URL ban đầu vào folder *downloads* như đề bài yêu cầu, ngoại trừ file *Divvy_Trips_2220_Q1.zip* là bị lỗi không giải nén được. ![](/images/ex1.png) |
| Exercise-2 | Nhóm đã hoàn thành file *main.py* bằng cách sử dụng các thư viện:<br>- *requests* để tải nội dung trang web;<br>- *BeautifulSoup* để phân tích cấu trúc HTML và tìm file có thời gian "2024-01-19 10:27";<br>- *pandas* để đọc file CSV và tìm giá trị lớn nhất của "HourlyDryBulbTemperature".<br>Các hàm chính nhóm đã xây dựng:<br>- *make_download_dir* để tạo thư mục downloads nếu chưa tồn tại;<br>- *download_file* để tải file về từ URL tìm được;<br>- *highest_HourlyDryBulbTemperature* để tìm giá trị nhiệt độ cao nhất;<br>- *main* để điều phối toàn bộ quy trình từ tìm kiếm đến hiển thị kết quả.<br>Kết quả chạy code đã tìm đúng file và xác định được giá trị nhiệt độ cao nhất như yêu cầu. ![](/images/ex2.png) |
| Exercise-3 | Nhóm đã giải quyết bài tập bằng cách sử dụng:<br>- *requests* để tải file từ AWS Common Crawl mà không cần dùng boto3;<br>- *gzip* và *io* để xử lý file nén trong bộ nhớ;<br>- Xử lý encoding UTF-8 để đảm bảo hiển thị đúng các ký tự đặc biệt.<br>Các hàm quan trọng:<br>- *download_and_extract_gz_in_memory* tải và giải nén file gz đầu tiên;<br>- *stream_gz_lines* đọc file thứ 2 theo từng dòng để tiết kiệm bộ nhớ;<br>- *main* điều phối quá trình và xử lý lỗi.<br>Code đã chạy thành công, tải được file và in ra nội dung từng dòng của file lớn mà không gặp vấn đề về bộ nhớ. ![](/images/ex3.png) |
| Exercise-4 | Nhóm đã hoàn thành bài tập với các thư viện:<br>- *glob* để tìm kiếm đệ quy các file JSON;<br>- *json* để đọc dữ liệu từ file;<br>- *csv* để ghi dữ liệu đã được làm phẳng.<br>Các hàm chính:<br>- *find_all_json_files* tìm tất cả file JSON trong thư mục data;<br>- *flatten_json* xử lý làm phẳng cấu trúc JSON phức tạp, đặc biệt với trường hợp tọa độ địa lý;<br>- *convert_json_to_csv* chuyển đổi từ JSON sang CSV;<br>- *main* điều phối toàn bộ quá trình.<br>Code đã chạy thành công, tìm và chuyển đổi tất cả file JSON sang CSV với cấu trúc dữ liệu phẳng. ![](/images/ex4.png) |
| Exercise-5 | Nhóm đã xây dựng hệ thống cơ sở dữ liệu Postgres với:<br>- *psycopg2* để kết nối và tương tác với Postgres;<br>- Thiết kế schema với đầy đủ khóa chính, khóa ngoại và indexes;<br>- Cơ chế import dữ liệu từ CSV vào các bảng tương ứng.<br>Các thành phần chính:<br>- *create_tables* tạo 3 bảng customers, products, transactions với đầy đủ ràng buộc;<br>- *ingest_data* nhập dữ liệu từ các file CSV vào các bảng;<br>- *main* thiết lập kết nối và điều phối quá trình.<br>Hệ thống đã chạy thành công, tạo đúng cấu trúc bảng và nhập đầy đủ dữ liệu. ![](/images/ex5.png) |
| Pipeline | Nhóm thống nhất sử dụng Airflow để triển khai pipeline, dùng PostgreSQL để làm cơ sở dữ liệu. Trước tiên, nhóm đã xây dựng file docker-compose để định nghĩa các service có liên quan tới container, nhóm thống nhất định nghĩa service postgres của Exercise-5 vào chúng file docker-compose cho pipeline. Có 1 điểm khác biệt so với khi làm bài tập cá nhân là nhóm đã giới hạn lại số URL để tải file ở Exercise-1 xuống còn 1, ở Exercise-3 nhóm giới hạn chỉ in ra 10000 dòng của file thứ 2 để đỡ tốn thời gian. Sau đó chạy container của pipeline ![](/images/pipeline.png) |