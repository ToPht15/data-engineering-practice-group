### Báo cáo kết quả các bài tập đã làm được của nhóm 
| Bài tập | Báo cáo |
|:---|:---|
| Exercise-1 | Các thành viên đã hoàn chỉnh file *main.py* trong folder của bài tập này. Khi chạy code thì đã tải và giải nén thành công các file trong các URL ban đầu vào folder *downloads* như đề bài yêu cầu, ngoại trừ file *Divvy_Trips_2220_Q1.zip* là bị lỗi không giải nén được. |
| Exercise-2 | Các thành viên trong nhóm tìm URL chứa file ghi thông tin khí tượng tại *10:27 ngày 19-01-2024* bằng cách sử dụng *BeautifulSoup* để phân tích cú pháp HTML trả về, tìm được file ghi thông tin khí tượng tại *10:27 ngày 19-01-2024* trong 1 tag \<tr> bên trong \<table>. Sau đó tải về và dùng *Pandas* giải quyết yêu cầu của đề bài. |
| Exercise-3 | Ở bài tập này các thành viên đã dùng các thư viện phù hợp để tải file thứ nhất về tìm URL và tải file thứ 2 về, tuy nhiên file này có chứa nhiều bảng mã của các ngôn ngữ khác nhau nên quá trình giải mã khá phức tạp. Quá trình in ra nội dung của file thứ 2 khá lâu vì file có dung lượng lớn. |
| Exercise-4 | Các thành viên đã sử dụng các thư viện phù hợp để tìm các file *.json* trong folder *data*, sau đó chuyển chúng sang file *.csv* bằng các phép biến đổi. |
| Exercise-5 | Đối với bài tập này, các thành viên đã vận dụng kiến thức, kĩ năng về *Docker* kết hợp với *truy vấn cơ sở dữ liệu* để viết các hàm *kết nối, tạo bảng, ghi dữ liệu* vào cơ sở dữ liệu trên *Postgres* bằng cách sử dụng các *thư viện Python* phù hợp. |
| Pipeline | Nhóm thống nhất sử dụng Airflow để triển khai pipeline, dùng PostgreSQL để làm cơ sở dữ liệu. Trước tiên, nhóm đã xây dựng file docker-compose để định nghĩa các service có liên quan tới container, nhóm thống nhất định nghĩa service postgres của Exercise-5 vào chúng file docker-compose cho pipeline. Có 1 điểm khác biệt so với khi làm bài tập cá nhân là nhóm đã giới hạn lại số URL để tải file ở Exercise-1 xuống còn 1, ở Exercise-3 nhóm giới hạn chỉ in ra 10000 dòng của file thứ 2 để đỡ tốn thời gian. Sau đó chạy container của pipeline 1 ![](/images/result.png) |